{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting User Satisfaction with Amazon Alexa\n",
    "## Logistic Regression + TF-IDF and BERT encoding for the Star Rating Prediction \n",
    "### By Elena Korshakova and Diedre Brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook compares the performance of the TF-IDF encoding method with the BERT encoding method in combination with the generalized linear model such as logistic regression. We aim to compare the performance of different encoding methods based on the accuracy and F-1 score because ratings were unbalanced (skewed towards 5-star score). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"data/df_train.pickle\")\n",
    "test = pd.read_pickle(\"data/df_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great speaker</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great little</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awesome</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great device</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6850</th>\n",
       "      <td>fun love</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6851</th>\n",
       "      <td>lot fun</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6852</th>\n",
       "      <td>buy gift husband problem set want return past ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6853</th>\n",
       "      <td>set control light home thermostat love able se...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6854</th>\n",
       "      <td>shame try friend excited try voice training he...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6765 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  rating\n",
       "0                                         great speaker       3\n",
       "1                                          great little       4\n",
       "2                                               awesome       5\n",
       "3                                                  love       5\n",
       "4                                          great device       5\n",
       "...                                                 ...     ...\n",
       "6850                                           fun love       5\n",
       "6851                                            lot fun       5\n",
       "6852  buy gift husband problem set want return past ...       3\n",
       "6853  set control light home thermostat love able se...       5\n",
       "6854  shame try friend excited try voice training he...       2\n",
       "\n",
       "[6765 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic Regression + TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform reviews into features (TF-IDF encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started to transform the reviews into features using the TF-IDF as term-weighting method intended to reflect how important a word is in the review. In our case the TF–IDF value increases proportionally to the number of times a word appears in the review and is offset by the number of reviews in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfrom training data\n",
    "X = vectoriser.fit_transform(train['review'])\n",
    "y = train['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6765, 3625)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data\n",
    "X_test = vectoriser.transform(test['review'])\n",
    "y_test = test['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3039, 3625)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning (Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best paremetrs for our data we started with the hyperparametr tuning using GridSearchCV to evaluate all the possible combinations of parameter values and retain the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.15, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a parameter grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'C': np.arange(0.1, 5, 0.2),\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'class_weight': ['balanced']   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search object\n",
    "clf = GridSearchCV(LogisticRegression(max_iter = 500), param_grid = param_grid, cv = 5, n_jobs=-1)\n",
    "\n",
    "# Fit on data\n",
    "best_clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.30000000000000004,\n",
       " 'class_weight': 'balanced',\n",
       " 'penalty': 'l2',\n",
       " 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val = model.predict(X_val)\n",
    "preds_train = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy score:  0.7689\n",
      "Validation accuracy score:  0.7133\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy score: \", np.round(accuracy_score(y_train, preds_train), 4))\n",
    "print(\"Validation accuracy score: \", np.round(accuracy_score(y_val, preds_val), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 score:  0.7566\n",
      "Validation F1 score:  0.7001\n"
     ]
    }
   ],
   "source": [
    "print(\"Training F1 score: \", np.round(f1_score(y_train, preds_train, average='weighted'), 4))\n",
    "print(\"Validation F1 score: \", np.round(f1_score(y_val, preds_val, average='weighted'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we got 77% traning accuracy (F1=76%) and 71% validation accuracy (F1=70%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refit the best model and predict on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the hyperparametr tuning results we refit the model on the 2017 dataset and predict on 2018 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.30000000000000004, class_weight='balanced', max_iter=500,\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Refit the model on the full training set\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the test set:  0.6779\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score on the test set: \", np.round(accuracy_score(y_test, preds_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on the test set:  0.6834\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score on the test set: \", np.round(f1_score(y_test, preds_test, average='weighted'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We` got 68% accuracy (F1=68) using TF-IDF encoding and logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression + BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step aims to increase the accuracy using BERT encoding method. BERT considers all the words of the input reviews simultaneously and then uses an attention mechanism to develop a contextual meaning of the words within each review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform reviews into features (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great speaker</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great little</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>awesome</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great device</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6850</th>\n",
       "      <td>fun love</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6851</th>\n",
       "      <td>lot fun</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6852</th>\n",
       "      <td>buy gift husband problem set want return past ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6853</th>\n",
       "      <td>set control light home thermostat love able se...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6854</th>\n",
       "      <td>shame try friend excited try voice training he...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6765 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  rating\n",
       "0                                         great speaker       3\n",
       "1                                          great little       4\n",
       "2                                               awesome       5\n",
       "3                                                  love       5\n",
       "4                                          great device       5\n",
       "...                                                 ...     ...\n",
       "6850                                           fun love       5\n",
       "6851                                            lot fun       5\n",
       "6852  buy gift husband problem set want return past ...       3\n",
       "6853  set control light home thermostat love able se...       5\n",
       "6854  shame try friend excited try voice training he...       2\n",
       "\n",
       "[6765 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embedding = BertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(review):\n",
    "    \"\"\"Return mean of word embeddings for a reivew\"\"\"\n",
    "    row_embeddings = bert_embedding(review.split('/n'))[0][1:]\n",
    "    avg_embedding = np.mean(row_embeddings, axis=1)[0].tolist()\n",
    "    return avg_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 18min 39s, sys: 4min 16s, total: 1h 22min 56s\n",
      "Wall time: 19min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get embeddings for training set\n",
    "X_emb = np.array(list(train['review'].apply(get_embedding)))\n",
    "y_emb = train['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6765, 768)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train features to pickle file\n",
    "train_emb = pd.DataFrame(X_emb)\n",
    "train_emb['y'] = y_emb\n",
    "train_emb.to_pickle(\"data/df_train_emb.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32min 11s, sys: 1min 39s, total: 33min 50s\n",
      "Wall time: 7min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get embeddings for test set\n",
    "X_emb_test = np.array(list(test['review'].apply(get_embedding)))\n",
    "y_emb_test = test['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3039, 768)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_emb_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test features to pickle file\n",
    "test_emb = pd.DataFrame(X_emb_test)\n",
    "test_emb['y'] = y_emb_test\n",
    "test_emb.to_pickle(\"data/df_test_emb.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning (Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using exactly the same schema for the logistic regression to compare encoding performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make validation split\n",
    "X_train_emb, X_val_emb, y_train_emb, y_val_emb = train_test_split(X_emb, y_emb, test_size = 0.15, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5750, 768)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a parameter grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'C': np.arange(0.1, 5, 0.5),\n",
    "    'solver': ['saga'],\n",
    "    'class_weight': ['balanced']   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search object\n",
    "clf = GridSearchCV(LogisticRegression(max_iter = 100), param_grid = param_grid, cv = 5, n_jobs=-1)\n",
    "\n",
    "# Fit on data\n",
    "best_clf = clf.fit(X_train_emb, y_train_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.1, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'saga'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val = model.predict(X_val_emb)\n",
    "preds_train = model.predict(X_train_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy score:  0.7303\n",
      "Validation accuracy score:  0.5675\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy score: \", np.round(accuracy_score(y_train_emb, preds_train), 4))\n",
    "print(\"Validation accuracy score: \", np.round(accuracy_score(y_val_emb, preds_val), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 score:  0.744\n",
      "Validation F1 score:  0.6094\n"
     ]
    }
   ],
   "source": [
    "print(\"Training F1 score: \", np.round(f1_score(y_train_emb, preds_train, average='weighted'), 4))\n",
    "print(\"Validation F1 score: \", np.round(f1_score(y_val_emb, preds_val, average='weighted'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refit the best model and predict on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.1, class_weight='balanced', solver='saga')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Refit the model on the full training set\n",
    "model.fit(X_emb, y_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.predict(X_emb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the test set:  0.4962\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score on the test set: \", np.round(accuracy_score(y_emb_test, preds_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on the test set:  0.5635\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score on the test set: \", np.round(f1_score(y_emb_test, preds_test, average='weighted'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result BERT embeddings didn't give a boost in performance and we got only 49% accuracy (F-1=56%). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
